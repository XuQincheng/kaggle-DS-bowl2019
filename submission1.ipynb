{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/session_df.csv'),\n",
       " PosixPath('data/data-science-bowl-2019.zip'),\n",
       " PosixPath('data/specs.csv'),\n",
       " PosixPath('data/train_labels.csv'),\n",
       " PosixPath('data/train.csv'),\n",
       " PosixPath('data/sample_submission.csv'),\n",
       " PosixPath('data/test.csv')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(path.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_df = pd.read_csv(path/'session_df.csv',index_col=0)\n",
    "train_labels = pd.read_csv(path/'train_labels.csv')\n",
    "test_data = pd.read_csv(path/'test.csv')\n",
    "specs = pd.read_csv(path/'specs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the size of the data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to reduce the DF size\n",
    "## Reference https://www.kaggle.com/caesarlupum/ds-bowl-start-here-a-gentle-introduction\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  0.49 Mb (48.2% reduction)\n",
      "Mem. usage decreased to 79.40 Mb (18.2% reduction)\n",
      "Mem. usage decreased to  0.01 Mb (0.0% reduction)\n",
      "Mem. usage decreased to 3994.70 Mb (23.2% reduction)\n"
     ]
    }
   ],
   "source": [
    "# train_data = reduce_mem_usage(train_data)\n",
    "train_labels = reduce_mem_usage(train_labels)\n",
    "test_data = reduce_mem_usage(test_data)\n",
    "specs = reduce_mem_usage(specs)\n",
    "session_df = reduce_mem_usage(session_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to decompose dates into more relevant parts\n",
    "def explode_date(col):\n",
    "    dayofyear = col.dt.dayofyear\n",
    "    weekofyear = col.dt.weekofyear\n",
    "    weekday = col.dt.weekday\n",
    "    month = col.dt.month\n",
    "    year = col.dt.year\n",
    "    hour = col.dt.hour\n",
    "    quarter = col.dt.quarter\n",
    "    return pd.DataFrame({'dayofyear':dayofyear,\n",
    "                         'weekofyear':weekofyear,\n",
    "                         'weekday':weekday,\n",
    "                         'quarter':quarter,\n",
    "                         'month':month,\n",
    "                         'year':year,\n",
    "                         'hour':hour})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main funtion that aggregates the sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(df,group_by_col):\n",
    "    \n",
    "    first_col = df.groupby(group_by_col,as_index=False)\\\n",
    "                  .count()\\\n",
    "                  .loc[:,group_by_col]\n",
    "    \n",
    "    # Number of events\n",
    "    event_counts = df.groupby(group_by_col)\\\n",
    "                             .agg({'game_session':'count'})\\\n",
    "                             .rename(columns={'game_session':'num_events'})\\\n",
    "                             .reset_index()\n",
    "    # flag to last event\n",
    "    last_event = df.groupby(group_by_col,as_index=False)\\\n",
    "                            .agg({'timestamp':'max'})\n",
    "    last_event['last_event_flag'] = 1\n",
    "    session_df_last_event_flagged = df.merge(last_event,on=[group_by_col,'timestamp'],how='left')\n",
    "    # get information about the assessment being predicted (title and world)\n",
    "    assessment_title = session_df_last_event_flagged\\\n",
    "                          .loc[session_df_last_event_flagged.last_event_flag == 1,'title']\n",
    "    assessment_world = session_df_last_event_flagged\\\n",
    "                          .loc[session_df_last_event_flagged.last_event_flag==1,'world']\n",
    "    \n",
    "    # get number of types of events (ie clips, activities,...)\n",
    "    type_count = df.groupby([group_by_col,'type'],as_index=False)\\\n",
    "                           .agg({'timestamp':'count'})\\\n",
    "                           .rename(columns={'timestamp':'num'})\\\n",
    "                           .pivot(index=group_by_col,columns='type', values='num',)\\\n",
    "                           .fillna(0)\\\n",
    "                           .reset_index()\n",
    "    # Counts number of sessions associated with each type\n",
    "    type_session_count = df.groupby([group_by_col,'type'],as_index=False)\\\n",
    "                                   .agg({'game_session':'nunique'})\\\n",
    "                                   .rename({'game_session':'ct'})\\\n",
    "                                   .pivot(index=group_by_col,columns='type',values='game_session')\\\n",
    "                                   .fillna(0)\\\n",
    "                                   .reset_index()\n",
    "\n",
    "    # Get time information about when the assessment was started\n",
    "    assessment_ts = session_df_last_event_flagged\\\n",
    "                          .loc[session_df_last_event_flagged.last_event_flag == 1,'timestamp']\n",
    "    assessment_ts_explode = explode_date(pd.to_datetime(assessment_ts))\n",
    "    \n",
    "    # Explode event type\n",
    "    event_id_counts = session_df.groupby(['session','event_id'],as_index=False)\\\n",
    "                            .agg({'timestamp':'count'})\\\n",
    "                            .pivot(index='session',columns='event_id',values='timestamp')\\\n",
    "                            .fillna(0)\n",
    "    \n",
    "    event_id_counts.columns = [col+'_count' for col in event_id_counts.columns]\n",
    "\n",
    "\n",
    "    out = pd.DataFrame({'installation_id':first_col.to_numpy(),\n",
    "                        'event_counts':event_counts.num_events.to_numpy(),\n",
    "                        'assessment_title':assessment_title.to_numpy(),\n",
    "                        'assessment_world':assessment_world.to_numpy(),\n",
    "                        'activity_count_events':type_count['Activity'].to_numpy(),\n",
    "                        'assessment_count_events':type_count['Assessment'].to_numpy(),\n",
    "                        'clip_count_events':type_count['Clip'].to_numpy(),\n",
    "                        'game_count_events':type_count['Game'].to_numpy(),\n",
    "                        'activity_count_sessions':type_session_count['Activity'].to_numpy(),\n",
    "                        'assessment_count_sessions':type_session_count['Assessment'].to_numpy(),\n",
    "                        'clip_count_sessions':type_session_count['Clip'].to_numpy(),\n",
    "                        'game_count_sessions':type_session_count['Game'].to_numpy(),\n",
    "                        'dayofyear_assess':assessment_ts_explode.dayofyear.to_numpy(),\n",
    "                        'weekofyear_assess':assessment_ts_explode.weekofyear.to_numpy(),\n",
    "                        'weekday_assess':assessment_ts_explode.weekday.to_numpy(),\n",
    "                        'quarter_assess':assessment_ts_explode.quarter.to_numpy(),\n",
    "                        'month_assess':assessment_ts_explode.month.to_numpy(),\n",
    "                        'hour_assess':assessment_ts_explode.hour.to_numpy()})\n",
    "    \n",
    "    return pd.concat([out,event_id_counts],axis=1)\n",
    "    \n",
    "seq_agg = aggregate(session_df,'session')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate labels to features\n",
    "train_data_agg = pd.concat([seq_agg,train_labels.accuracy_group],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save aggregated data as csv\n",
    "train_data_agg.to_csv(path/'data_agg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.metrics import cohen_kappa_score,confusion_matrix, make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_agg = pd.read_csv(path/'data_agg.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(train_data_agg,test_size=.2)\n",
    "train_x = train.iloc[:,:-1]\n",
    "train_y = train.iloc[:,-1]\n",
    "test_x = test.iloc[:,:-1]\n",
    "test_y = test.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([('drop','drop',['installation_id','dayofyear_assess',\n",
    "                                                  'weekofyear_assess','weekday_assess',\n",
    "                                                  'quarter_assess','month_assess',\n",
    "                                                  'hour_assess']),\n",
    "                                  ('onehot',OneHotEncoder(),['assessment_title',\n",
    "                                                             'assessment_world'])],\n",
    "                                 remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('col_transform',preprocessor),\n",
    "                 ('cls',RandomForestClassifier(n_estimators=100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'cls__n_estimators':range(1,300,10),\n",
    "          'cls__max_depth':range(1,15,2),\n",
    "          'cls__min_samples_leaf':range(10,50,2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/kgloftis/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('col_transform',\n",
       "                                              ColumnTransformer(n_jobs=None,\n",
       "                                                                remainder='passthrough',\n",
       "                                                                sparse_threshold=0.3,\n",
       "                                                                transformer_weights=None,\n",
       "                                                                transformers=[('drop',\n",
       "                                                                               'drop',\n",
       "                                                                               ['installation_id',\n",
       "                                                                                'dayofyear_assess',\n",
       "                                                                                'weekofyear_assess',\n",
       "                                                                                'weekday_assess',\n",
       "                                                                                'quarter_assess',\n",
       "                                                                                'month_assess...\n",
       "                                                                     n_jobs=None,\n",
       "                                                                     oob_score=False,\n",
       "                                                                     random_state=None,\n",
       "                                                                     verbose=0,\n",
       "                                                                     warm_start=False))],\n",
       "                                      verbose=False),\n",
       "                   iid='warn', n_iter=10, n_jobs=12,\n",
       "                   param_distributions={'cls__max_depth': range(1, 15, 2),\n",
       "                                        'cls__min_samples_leaf': range(10, 50, 2),\n",
       "                                        'cls__n_estimators': range(1, 300, 10)},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter tuning and cross validation\n",
    "cv = RandomizedSearchCV(pipe,params,n_jobs=12,n_iter=10)\n",
    "cv.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20545560745040026"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = cv.predict(test_x)\n",
    "cohen_kappa_score(test_y,preds,weights='quadratic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Kaggle Submission (work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_submission():\n",
    "#     test_agg = aggregate(test_data,'installation_id')\n",
    "#     test_predictions = cv.predict(test_agg)\n",
    "#     submission_df = pd.DataFrame({'installation_id':test_agg.installation_id.to_numpy(),\n",
    "#                                   'accuracy_group':test_predictions})\n",
    "#     return submission_df\n",
    "# test = prepare_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.to_csv('submission/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
