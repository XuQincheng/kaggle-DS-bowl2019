{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019 Data Science Bowl -- PBS Kids Measure Up\n",
    "\n",
    "\n",
    "## Project Notebook\n",
    "\n",
    "Team name:  Anonymous Chameleon\n",
    "\n",
    "Team Members: Ariana Moncada, Hoda Noorian, Kevin Loftis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from plotnine import *\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in train.csv\n"
     ]
    }
   ],
   "source": [
    "print('Reading in train.csv')\n",
    "train_data = pd.read_csv(path/'train.csv')\n",
    "print('Reading in train_labels.csv')\n",
    "train_labels = pd.read_csv(path/'train_labels.csv')\n",
    "print('Reading in test.csv')\n",
    "test_data = pd.read_csv(path/'test.csv')\n",
    "print('Reading in specs.csv')\n",
    "specs = pd.read_csv(path/'specs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_description_dict = {row.event_id:row.info for idx,row in specs.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_titles = set(train_data.title.unique()).union(set(test_data.title.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduces data with respect to each installation id\n",
    "def reduce_data_per_user(user_dataframe):\n",
    "    '''\n",
    "    Helper function to aggregate data.  To be used inside the function to_reduce.\n",
    "    This function expects as input a dataframe with all rows pertaining to a given\n",
    "    installation id.  The data frame is then grouped by game session and features are\n",
    "    aggregated.  Features include total number events, number of events types, game session\n",
    "    length, etc.  \n",
    "    \n",
    "    If the game session is an 'Assessment' session then the current counts are recorded as a row\n",
    "    in the aggregated dataset.  Since the game session ids in the labels only correspond to the\n",
    "    Assessment session, we only want to record accumulated features just before the assessment.  This\n",
    "    gives an estimate of the state of the user just before they take an assessment which we then use\n",
    "    for prediction.\n",
    "    '''\n",
    "    event_counts = {'total_events':0,\n",
    "                    'clips_count':0,\n",
    "                    'assessment_count':0,\n",
    "                    'activity_count':0,\n",
    "                    'game_count':0,\n",
    "                    'correct_events_count':0,\n",
    "                    'incorrect_events_count':0\n",
    "                   }\n",
    "    \n",
    "    specific_event_counts = dict(zip(specs.event_id,[0 for _ in range(specs.shape[0])]))\n",
    "    unique_titles_counts = dict(zip(unique_titles,[0 for _ in range(len(unique_titles))]))\n",
    "    median_session_times = []\n",
    "    df_features = []\n",
    "    for idx,session in user_dataframe.groupby('game_session',sort=False):\n",
    "        if session.type.iloc[0]=='Assessment':\n",
    "            assessment_info = {'assess_world':session.world.iloc[0],\n",
    "                               'assess_title':session.title.iloc[0],\n",
    "                               'game_session':session.game_session.iloc[0],\n",
    "                               'installation_id':session.installation_id.iloc[0]\n",
    "                              }\n",
    "            \n",
    "            df_features.append({**event_counts,**assessment_info,**specific_event_counts,\n",
    "                                'median_session_time':np.median(median_session_times),\n",
    "                                **unique_titles_counts})\n",
    "            event_counts['assessment_count'] += (session.type == 'Assessment').sum()\n",
    "            unique_titles_counts[session.iloc[0].title]+=1\n",
    "        else:\n",
    "            event_counts['total_events'] += session.event_id.count()\n",
    "            event_counts['clips_count'] += (session.type == 'Clip').sum()\n",
    "            event_counts['activity_count'] += (session.type == 'Activity').sum()\n",
    "            event_counts['game_count'] += (session.type == 'Game').sum()\n",
    "            event_counts['correct_events_count'] += (session.event_id\n",
    "                                                     .map(event_description_dict)\n",
    "                                                     .str.contains('Correct').sum())\n",
    "            event_counts['incorrect_events_count'] += (session.event_id\n",
    "                                                       .map(event_description_dict)\n",
    "                                                       .str.contains('Incorrect').sum())\n",
    "            specific_event_counts.update(dict(session.event_id.value_counts()))\n",
    "            unique_titles_counts[session.iloc[0].title]+=1\n",
    "            median_session_times.append(session.game_time.max())\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data(valid_install_ids,data,is_train_data=True):\n",
    "    '''\n",
    "    Main function that aggregates the data.\n",
    "    \n",
    "    Takes install_ids that are used for aggregating (note that there are some installation\n",
    "    ids that do not have a row in the labels) and aggregates.  If is_train_data is True, then\n",
    "    the aggregated table is merged with the labels, otherwise just the aggregated table is returned.\n",
    "    '''\n",
    "    rows = []\n",
    "    for install_id,df in tqdm(data.groupby('installation_id')):\n",
    "        if install_id in valid_install_ids: # only aggregate if install_id exists in train_labels\n",
    "            rows.extend(reduce_data_per_user(df))\n",
    "        else:\n",
    "            continue\n",
    "    data = pd.DataFrame(rows)\n",
    "    if is_train_data:\n",
    "        return train_labels[['game_session','accuracy_group']]\\\n",
    "                        .merge(data,on='game_session',how='inner')\\\n",
    "                        .drop(['game_session'],axis=1)\n",
    "    else:\n",
    "        return data.groupby('installation_id',as_index=False).apply(lambda x: x.iloc[-1]).drop(['game_session'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_with_constant_values(data):\n",
    "    '''\n",
    "    Given the aggregated dataset, returns the indicies of the\n",
    "    columns with a constant value.\n",
    "    '''\n",
    "    drop_cols = []\n",
    "    for i in range(data.shape[1]):\n",
    "        if data.iloc[:,i].nunique() == 1:\n",
    "            drop_cols.append(data.columns[i])\n",
    "    return drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cached_table = True # set to False if you want to recompute\n",
    "train_agg_path = path/'train_reduced.csv'\n",
    "test_agg_path = path/'test_reduced.csv'\n",
    "if use_cached_table:\n",
    "    train_reduced = pd.read_csv(train_agg_path,index_col=0)\n",
    "    test_reduced = pd.read_csv(test_agg_path,index_col=0)\n",
    "else:\n",
    "    train_reduced = reduce_data(train_labels.installation_id.unique(),train_data)\n",
    "    test_reduced = reduce_data(test_data.installation_id.unique(),test_data, is_train_data = False)\n",
    "    drop_cols = get_cols_with_constant_values(train_reduced) # get columns with constant values\n",
    "    train_reduced = train_reduced.drop(drop_cols,axis=1) # drop columns from train set with constant vals\n",
    "    test_reduced = test_reduced.drop(drop_cols,axis=1)   # drop columns from test set with constant vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tables\n",
    "train_reduced.to_csv(path/'train_reduced.csv')\n",
    "test_reduced.to_csv(path/'test_reduced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import cohen_kappa_score,confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worlds = list(set(train_reduced.assess_world.unique()).union(test_reduced.assess_world.unique()))\n",
    "titles = list(set(train_reduced.assess_title.unique()).union(test_reduced.assess_title.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_col_names = [col for col in train_reduced.columns if col not in ['accuracy_group','installation_id','median_session_time']]\n",
    "X = train_reduced.loc[:,X_col_names]\n",
    "y = train_reduced.loc[:,'accuracy_group']\n",
    "train_x,test_x, train_y,test_y = train_test_split(X,y,test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = ggplot(data=pd.DataFrame(train_y))\\\n",
    "        + geom_bar(aes(x='accuracy_group'),fill='dodgerblue')\\\n",
    "        + ggtitle('Distribution of labels in train set')\\\n",
    "        + geom_text(aes(x='accuracy_group',label='round(..prop..,2)'), stat='count',vjust='bottom')\\\n",
    "        + xlab('Count')\\\n",
    "        + ylab('Accuracy Group')\\\n",
    "        + theme_minimal()\n",
    "plot.draw();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About half of the observations fall into group 3.  To balance this we chose balance out the classes using SMOTENC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTENC([7,8])\n",
    "train_x_smote,train_y_smote = smote.fit_resample(train_x,train_y)\n",
    "train_x_smote = pd.DataFrame(train_x_smote,columns=X_col_names)\n",
    "\n",
    "train_x_smote['installation_id'] = 1\n",
    "train_x['installation_id'] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_before_after_smote_y_dist():\n",
    "    '''\n",
    "    ggplot code to plot distribution of labels before and after SMOTENC resampling\n",
    "    '''\n",
    "    df1, df2 = (pd.DataFrame({'accuracy_group':train_y}),\n",
    "                pd.DataFrame({'accuracy_group':train_y_smote}))\n",
    "    df1['smote'] = 'Original Data'\n",
    "    df2['smote'] = 'After SMOTE'\n",
    "    df = pd.concat([df1,df2],axis=0)\n",
    "    plot = (ggplot(data=df)+\n",
    "            geom_bar(aes(x='accuracy_group',fill='smote'),position=position_dodge())+\n",
    "            xlab('Accuracy Group')+\n",
    "            ylab('Number of Observations')+\n",
    "            theme_minimal()+\n",
    "            theme(legend_title = element_blank())+\n",
    "            scale_fill_manual(['dodgerblue','gainsboro']))\n",
    "    return plot\n",
    "plot = plot_before_after_smote_y_dist()\n",
    "plot.save(filename='./before_after_smote.png',dpi=400)\n",
    "plot.draw();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After SMOTENC resampling, the classes are balanced.  We will fit both the SMOTENC sampled data and on non-smote samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,x,y,n_jobs=4):\n",
    "    '''\n",
    "    helper function to evaluate models.  Compute the 5 fold cross validation and prints \n",
    "    report of averge CV fold and training set scores for the following metrics:\n",
    "        quadratic_cappa\n",
    "        weighted_f1\n",
    "        accuracy\n",
    "    '''\n",
    "    scorers = {'quadratic_cappa':make_scorer(cohen_kappa_score,weights='quadratic'),\n",
    "               'weighted_f1':make_scorer(metrics.f1_score,average='weighted'),\n",
    "               'accuracy':make_scorer(metrics.accuracy_score)}\n",
    "\n",
    "    CV = cross_validate(model,x,y,cv=5,n_jobs=n_jobs,scoring=scorers,return_train_score=True)\n",
    "    \n",
    "    train_cv_quadrtic_cappa_mean = CV['train_quadratic_cappa'].mean()\n",
    "    train_cv_quadrtic_cappa_std = CV['train_quadratic_cappa'].std()\n",
    "    test_cv_quadrtic_cappa_mean = CV['test_quadratic_cappa'].mean()\n",
    "    test_cv_quadrtic_cappa_std = CV['test_quadratic_cappa'].std()\n",
    "    \n",
    "    train_cv_weighted_f1_mean = CV['train_weighted_f1'].mean()\n",
    "    train_cv_weighted_f1_std = CV['train_weighted_f1'].std()\n",
    "    test_cv_weighted_f1_mean = CV['test_weighted_f1'].mean()\n",
    "    test_cv_weighted_f1_std = CV['test_weighted_f1'].std()\n",
    "    \n",
    "    train_cv_accuracy_mean = CV['train_accuracy'].mean()\n",
    "    train_cv_accuracy_std = CV['train_accuracy'].std()\n",
    "    test_cv_accuracy_mean = CV['test_accuracy'].mean()\n",
    "    test_cv_accuracy_std = CV['test_accuracy'].std()\n",
    "    \n",
    "    print(f'Quadratic Kappa mean Train: {train_cv_quadrtic_cappa_mean:.3f} std: {train_cv_quadrtic_cappa_std:.3f}')\n",
    "    print(f'Quadratic Kappa mean CV: {test_cv_quadrtic_cappa_mean:.3f} std: {test_cv_quadrtic_cappa_std:.3f}')\n",
    "    print()\n",
    "    print(f'Weighted F1 mean Train: {train_cv_weighted_f1_mean:.3f} std: {train_cv_weighted_f1_std:.3f}')\n",
    "    print(f'Weighted F1 mean CV: {test_cv_weighted_f1_mean:.3f} std: {test_cv_weighted_f1_std:.3f}')\n",
    "    print()\n",
    "    print(f'Accuracy mean Train: {train_cv_accuracy_mean:.3f} std: {train_cv_accuracy_std:.3f}')\n",
    "    print(f'Accuracy mean CV: {test_cv_accuracy_mean:.3f} std: {test_cv_accuracy_std:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([('onehot1',OneHotEncoder(),['assess_world']),\n",
    "                                  ('onehot2',OneHotEncoder(),['assess_title']),\n",
    "                                  ('drop', 'drop',['installation_id'])],\n",
    "                                  remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_pipe = Pipeline([('col_transform',preprocessor),\n",
    "                      ('cls',DummyClassifier(strategy='uniform'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a baseline model of random guesseing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Random Model results')\n",
    "print()\n",
    "evaluate_model(dumb_pipe,train_x,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected does not perform well, but provides a base to compare more complex models to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try lasso logistic regression for both SMOTENC resampled data and non-resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Lasso Logistic Regression results')\n",
    "lasso_pipe = Pipeline([('col_transform',preprocessor),\n",
    "                         ('cls',OneVsRestClassifier(lm.Lasso(normalize=True,max_iter=5000,alpha=2e-5)))])\n",
    "evaluate_model(lasso_pipe,train_x,train_y.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_pipe.fit(train_x,train_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Lasso Smote Regression results')\n",
    "lasso_smote_pipe = Pipeline([('col_transform',preprocessor),\n",
    "                         ('cls',OneVsRestClassifier(lm.Lasso(normalize=True,max_iter=5000,alpha=2e-5)))])\n",
    "evaluate_model(lasso_smote_pipe,train_x_smote,train_y_smote,n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the smote dataset actually performed worse for the lasso model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_smote_pipe.fit(train_x_smote,train_y_smote);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe = Pipeline([('col_transform',preprocessor),\n",
    "                 ('cls',RandomForestClassifier(n_estimators=250,\n",
    "                                               max_features = int(np.sqrt(train_x.shape[1])),\n",
    "                                               min_samples_leaf=10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Random Forest results')\n",
    "evaluate_model(rf_pipe,train_x,train_y,n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe.fit(train_x,train_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Random Forest SMOTE results')\n",
    "rf_smote_pipe = Pipeline([('col_transform',preprocessor),\n",
    "                 ('cls',RandomForestClassifier(n_estimators=250,\n",
    "                                               max_features = int(np.sqrt(train_x.shape[1])),\n",
    "                                               min_samples_leaf=10))])\n",
    "evaluate_model(rf_smote_pipe,train_x_smote,train_y_smote,n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTENC resampling help the Random Forest (RF) Model acheive a higher mean CV score, however the standard deviation of the scoring metrics are much higher than that of the RF model trained on the regular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_smote_pipe.fit(train_x_smote,train_y_smote);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pipe = Pipeline([('col_transform',preprocessor),\n",
    "                    ('cls',xgb.XGBClassifier(learning_rate=.01))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('XGBoost Classifier Results')\n",
    "evaluate_model(gb_pipe,train_x,train_y,n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pipe.fit(train_x,train_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_smote_pipe = Pipeline([('col_transform',preprocessor),\n",
    "                    ('cls',xgb.XGBClassifier(learning_rate=.01))])\n",
    "print('XGBoost Classifier SMOTE Results')\n",
    "evaluate_model(gb_pipe,train_x_smote,train_y_smote,n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_smote_pipe.fit(train_x_smote,train_y_smote);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'Lasso':lasso_pipe,'Lasso w/ SMOTE':lasso_smote_pipe,\n",
    "          'Random Forest':rf_pipe, 'Random Forest w/ SMOTE':rf_smote_pipe,\n",
    "          'XGBoost':gb_pipe, 'XGBoost w/ SMOTE':gb_smote_pipe}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models):\n",
    "    \"\"\"\n",
    "    Evaluates models on the testing holdout set to get a true measure of performance to\n",
    "    compare with the other models.\n",
    "    \"\"\"\n",
    "    for model_name,model in models.items():\n",
    "        predictions = model.predict(test_x)\n",
    "        kappa = cohen_kappa_score(predictions,test_y,weights='quadratic')\n",
    "        f1 = metrics.f1_score(predictions,test_y,average='weighted')\n",
    "        acc = metrics.accuracy_score(predictions,test_y)\n",
    "        print(f'{model_name} Results')\n",
    "        print(f'Quadratic Kappa: {kappa}')\n",
    "        print(f'Weighted F1 score: {f1}')\n",
    "        print(f'Accuracy F1 score: {acc}')\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model trained on the SMOTENC resample performed the best measured by the quadratic kappa.  For this point forward I will analyze the outputs of this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rfpimp import *\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions of winning model for model analysis\n",
    "predictions = rf_smote_pipe.predict(test_x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first take a look at the confusion matrix of the predictions on our hold out set of the RF model trained on SMOTENC resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(test_y,predictions),annot=True,\n",
    "            cmap='Blues',fmt='d',cbar=False)\n",
    "plt.ylabel('True Accuracy Groups')\n",
    "plt.xlabel('Predicted Accuracy Groups');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs better on individual that are part of accuacy group 0 (assessment never solved) and group 3 (assessment solve on first try).  However, the model has a hard time with groups 1(assessment solved in 3+ tries) and 2(assessment solved in 2 tries).\n",
    "\n",
    "The confusion matrix suggests that individuals of group 2 look extremely similar to those in group 3 with the majority of the true group 2 individuals predicted to be part of group 3. Users who get it in one try vs two tries are probably very similar.\n",
    "\n",
    "The model similarly had difficult predicting individuals in accuaracy group 1, bucketing the majority of true group ones in either group 0 or 3.  Interestingly, the model did not mispredict into 2 as often as one might think given that they are adjacent classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets examine which features are important to the model using permutation feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imps = importances(rf_smote_pipe,train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_imps = imps[:20].reset_index()\n",
    "top_20_imps['Feature'] = pd.Categorical(top_20_imps['Feature'],categories=top_20_imps['Feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = ggplot(data = top_20_imps) +\\\n",
    "            geom_bar(aes(x='Feature',y='Importance'),\n",
    "                     stat='identity',fill='dodgerblue')+\\\n",
    "            coord_flip()+\\\n",
    "            theme_minimal() +\\\n",
    "            labs(title='Feature Importance')\n",
    "plot.draw();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important feature according to the feature importance above is the assess_title feature.  This feature indicates which assessment the user is taking.  For some assessments individual probably do worse on average making this a string feature.\n",
    "\n",
    "In addition to assess title there are a few features with strange strings like b012cd7f.  These correspond the the number of a that events triggered prior to the start.  Let's take a look at a couple of the top event descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(specs.loc[specs.event_id == 'b012cd7f','info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above event description triggers when the user beats a round.  This make sense.  The more rounds beaten (or not beaten) seem like it would be a pretty good indicator or mastery of a skill that would probably be predictive of future performance on an assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(specs.loc[specs.event_id == 'e5734469','info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(specs.loc[specs.event_id == '58a0de5c','info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above two events above are interesting in that they are triggered when feedback (either correct or incorrect) is delivered.  Again similar to the 'beat round' event, the more correct or incorrect feedback given to a user would probably be indicative of their performance on future assessments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of of submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_submission(model,test_data):\n",
    "    '''\n",
    "    Function that prepares submission\n",
    "    '''\n",
    "    install_id = test_data.installation_id.to_numpy()\n",
    "    test_data = test_data.drop('installation_id',axis=1)  # put installation_id at the end to be consistant with train_set\n",
    "    test_data['installation_id'] = install_id             # otherwise the pipeline throws an error\n",
    "    test_predictions = model.predict(test_data)\n",
    "    submission_df = pd.DataFrame({'installation_id':test_data.installation_id.to_numpy(),\n",
    "                                  'accuracy_group':test_predictions})\n",
    "    return submission_df\n",
    "test = prepare_submission(rf_smote_pipe,test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
